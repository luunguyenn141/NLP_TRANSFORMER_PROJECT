import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_head):
        super(MultiHeadAttention, self).__init__()
        # d_model: KÃ­ch thÆ°á»›c vector cá»§a tá»« (vÃ­ dá»¥ 512)
        # n_head: Sá»‘ lÆ°á»£ng Ä‘áº§u (vÃ­ dá»¥ 8)
        
        # Kiá»ƒm tra xem d_model cÃ³ chia háº¿t cho sá»‘ head khÃ´ng
        assert d_model % n_head == 0, "d_model pháº£i chia háº¿t cho n_head"
        
        self.d_head = d_model // n_head # KÃ­ch thÆ°á»›c cá»§a má»—i head
        self.d_model = d_model
        self.n_head = n_head
        
        # CÃ¡c lá»›p Linear Ä‘á»ƒ táº¡o ra Q, K, V
        # CÃ´ng thá»©c: Linear(input_dim, output_dim)
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        
        # Lá»›p Linear cuá»‘i cÃ¹ng Ä‘á»ƒ tá»•ng há»£p káº¿t quáº£
        self.w_o = nn.Linear(d_model, d_model)

    def forward(self, q, k, v, mask=None):
        # q, k, v cÃ³ kÃ­ch thÆ°á»›c: (batch_size, seq_len, d_model)
        batch_size = q.size(0)

        # --- BÆ¯á»šC 1: TÃNH Q, K, V VÃ€ TÃCH HEAD ---
        # 1. Qua lá»›p Linear
        # 2. Reshape Ä‘á»ƒ tÃ¡ch thÃ nh n_head
        # 3. Transpose Ä‘á»ƒ Ä‘áº£o n_head lÃªn trÆ°á»›c seq_len Ä‘á»ƒ tÃ­nh toÃ¡n song song
        # KÃ­ch thÆ°á»›c sau cÃ¹ng: (batch_size, n_head, seq_len, d_head)
        
        Q = self.w_q(q).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)
        K = self.w_k(k).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)
        V = self.w_v(v).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)

        # --- BÆ¯á»šC 2: TÃNH SCALED DOT-PRODUCT ATTENTION ---
        # CÃ´ng thá»©c: softmax(Q * K^T / sqrt(d_k))
        
        # NhÃ¢n ma tráº­n Q vá»›i K chuyá»ƒn vá»‹ (transpose 2 chiá»u cuá»‘i)
        scores = torch.matmul(Q, K.transpose(-2, -1))
        
        # Chia cho cÄƒn báº­c 2 cá»§a d_head Ä‘á»ƒ á»•n Ä‘á»‹nh gradient
        scores = scores / math.sqrt(self.d_head)

        # --- BÆ¯á»šC 3: ÃP Dá»¤NG MASK (QUAN TRá»ŒNG) ---
        # Náº¿u cÃ³ mask, ta gÃ¡n giÃ¡ trá»‹ táº¡i vá»‹ trÃ­ mask=0 thÃ nh Ã¢m vÃ´ cÃ¹ng (-1e9)
        # Äá»ƒ khi qua Softmax nÃ³ sáº½ báº±ng 0 (nghÄ©a lÃ  khÃ´ng thÃ¨m nhÃ¬n vÃ o tá»« Ä‘Ã³)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # --- BÆ¯á»šC 4: SOFTMAX VÃ€ NHÃ‚N Vá»šI V ---
        attn_weights = torch.softmax(scores, dim=-1)
        
        # Output shape: (batch_size, n_head, seq_len, d_head)
        output = torch.matmul(attn_weights, V)

        # --- BÆ¯á»šC 5: GHÃ‰P CÃC HEAD Láº I (CONCAT) ---
        # Äáº£o láº¡i vá»‹ trÃ­ dimensions: (batch_size, seq_len, n_head, d_head)
        # Sau Ä‘Ã³ Ã©p pháº³ng (flatten) 2 chiá»u cuá»‘i láº¡i thÃ nh d_model
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # --- BÆ¯á»šC 6: QUA Lá»šP LINEAR CUá»I ---
        return self.w_o(output)

# ==========================================
# PHáº¦N CODE Äá»‚ KIá»‚M TRA (UNIT TEST)
# ==========================================
if __name__ == "__main__":
    # Giáº£ láº­p tham sá»‘
    d_model = 512
    n_head = 8
    seq_len = 10
    batch_size = 2

    print("--- ÄANG KIá»‚M TRA MODULE ATTENTION ---")
    try:
        # Táº¡o dá»¯ liá»‡u giáº£ ngáº«u nhiÃªn
        x = torch.randn(batch_size, seq_len, d_model)
        
        # Khá»Ÿi táº¡o mÃ´ hÃ¬nh
        attention = MultiHeadAttention(d_model, n_head)
        
        # Cháº¡y thá»­
        output = attention(x, x, x) # Self-attention
        
        print(f"âœ… KÃ­ch thÆ°á»›c Ä‘áº§u vÃ o: {x.shape}")
        print(f"âœ… KÃ­ch thÆ°á»›c Ä‘áº§u ra:  {output.shape}")
        
        if output.shape == (batch_size, seq_len, d_model):
            print("ğŸ‰ CHÃšC Má»ªNG! Code cá»§a báº¡n Ä‘Ã£ cháº¡y Ä‘Ãºng.")
        else:
            print("âŒ Lá»–I: KÃ­ch thÆ°á»›c Ä‘áº§u ra khÃ´ng Ä‘Ãºng.")
            
    except Exception as e:
        print(f"âŒ Lá»–I CODE: {e}")