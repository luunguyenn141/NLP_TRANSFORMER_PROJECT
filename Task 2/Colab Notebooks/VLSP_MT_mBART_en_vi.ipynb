{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BS8PHdKe1nqB"},"outputs":[],"source":["!pip install -q \"transformers\u003e=4.44.0\" datasets sacrebleu evaluate accelerate sentencepiece\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3XMZY8W07iv2"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-907070862.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--\u003e 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DC3qVQLnJVkG"},"outputs":[],"source":["BASE_DIR = \"/content/drive/MyDrive/vlsp_mt_share\"\n","\n","TRAIN_EN_PATH    = f\"{BASE_DIR}/train.en.txt\"\n","TRAIN_VI_PATH    = f\"{BASE_DIR}/train.vi.txt\"\n","TRAIN_TSV        = f\"{BASE_DIR}/train_envi.tsv\"\n","TRAIN_SPLIT_TSV  = f\"{BASE_DIR}/train_envi_train.tsv\"\n","DEV_SPLIT_TSV    = f\"{BASE_DIR}/train_envi_dev.tsv\"\n","\n","MODEL_DIR        = f\"{BASE_DIR}/opus_envi_vlsp_best\"\n","PUBLIC_TEST_EN   = f\"{BASE_DIR}/public_test.en.txt\"\n","PUBLIC_TEST_VI   = f\"{BASE_DIR}/public_test.vi.txt\"\n","RESULTS_TSV      = f\"{BASE_DIR}/public_test_results_en_vi.tsv\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YTVPeJFY7rHH"},"outputs":[],"source":["with open(TRAIN_EN_PATH, encoding=\"utf-8\") as f:\n","    en_lines = [l.strip() for l in f.readlines()]\n","with open(TRAIN_VI_PATH, encoding=\"utf-8\") as f:\n","    vi_lines = [l.strip() for l in f.readlines()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xqTRnbcC-2rM"},"outputs":[],"source":["en_path = \"/content/drive/MyDrive/vlsp_mt_share/train.en.txt\"   # sửa nếu path khác\n","vi_path = \"/content/drive/MyDrive/vlsp_mt_share/train.vi.txt\"   # path em đã có\n","\n","with open(en_path, encoding=\"utf-8\") as f:\n","    en_lines = [l.strip() for l in f.readlines()]\n","\n","with open(vi_path, encoding=\"utf-8\") as f:\n","    vi_lines = [l.strip() for l in f.readlines()]\n","\n","print(\"Số dòng EN:\", len(en_lines))\n","print(\"Số dòng VI:\", len(vi_lines))\n","print(\"EN mẫu:\", en_lines[0])\n","print(\"VI mẫu:\", vi_lines[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vq-HwtC2KCcv"},"outputs":[],"source":["!pip install -q \"transformers==4.57.3\" datasets sacrebleu evaluate sentencepiece\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LYXk_Pxb-0zN"},"outputs":[],"source":["import pandas as pd\n","\n","with open(TRAIN_EN_PATH, encoding=\"utf-8\") as f:\n","    en_lines = [l.strip() for l in f.readlines()]\n","\n","with open(TRAIN_VI_PATH, encoding=\"utf-8\") as f:\n","    vi_lines = [l.strip() for l in f.readlines()]\n","\n","print(\"Số dòng EN:\", len(en_lines))\n","print(\"Số dòng VI:\", len(vi_lines))\n","print(\"EN mẫu:\", en_lines[0])\n","print(\"VI mẫu:\", vi_lines[0])\n","\n","df = pd.DataFrame({\n","    \"src\": en_lines,\n","    \"tgt\": vi_lines,\n","})\n","\n","df.to_csv(TRAIN_TSV, sep=\"\\t\", index=False)\n","print(\"Đã lưu:\", TRAIN_TSV, \"– số dòng:\", len(df))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yn5DgFJ9ARIN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 475000 Dev: 25000\n","Train TSV: /content/drive/MyDrive/vlsp_mt_share/train_envi_train.tsv\n","Dev TSV: /content/drive/MyDrive/vlsp_mt_share/train_envi_dev.tsv\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv(TRAIN_TSV, sep=\"\\t\")\n","\n","train_df, dev_df = train_test_split(df, test_size=0.05, random_state=42)\n","\n","train_df.to_csv(TRAIN_SPLIT_TSV, sep=\"\\t\", index=False)\n","dev_df.to_csv(DEV_SPLIT_TSV, sep=\"\\t\", index=False)\n","\n","print(\"Train:\", len(train_df), \"Dev:\", len(dev_df))\n","print(\"Train TSV:\", TRAIN_SPLIT_TSV)\n","print(\"Dev TSV:\", DEV_SPLIT_TSV)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mzBu_9ro-edE"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55bb733a28a34c2ea9e42e50d183c4ea","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8778ca530dda4660abc02927c9a48ff8","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['src', 'tgt'],\n","        num_rows: 475000\n","    })\n","    validation: Dataset({\n","        features: ['src', 'tgt'],\n","        num_rows: 25000\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","data_files = {\n","    \"train\": TRAIN_SPLIT_TSV,\n","    \"validation\": DEV_SPLIT_TSV,\n","}\n","\n","raw_datasets = load_dataset(\n","    \"csv\",\n","    data_files=data_files,\n","    delimiter=\"\\t\",\n",")\n","\n","raw_datasets\n"]},{"cell_type":"markdown","metadata":{"id":"B_mlRZfa-d8s"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R2iVIWyCKJzP"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db9647e4598449e1ab349864f61b3c30","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/44.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b4163f08d244d708f2cd2c7be319e75","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acc88a62a1ec4daab0fc662c00d486ab","version_major":2,"version_minor":0},"text/plain":["source.spm:   0%|          | 0.00/809k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"204fdbcfe14b41c3a697151dfd38d86c","version_major":2,"version_minor":0},"text/plain":["target.spm:   0%|          | 0.00/756k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a2b26da26cd433bb433f8acf138250c","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3b5373dc8de48f1aebb2207db385f20","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/289M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e577a537d254d6f9280e87bbdf25020","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_name = \"Helsinki-NLP/opus-mt-en-vi\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RxJ17GOtKUz3"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a58ba0912e1408cba5bb4dc0ec39bf7","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/475000 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94e85522dccd402c87baad9d9813e1d1","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/289M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbbb682c478e472ebd4c939998422079","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25000 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 475000\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 25000\n","    })\n","})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["max_source_length = 128\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = examples[\"src\"]\n","    targets = examples[\"tgt\"]\n","\n","    model_inputs = tokenizer(\n","        inputs,\n","        max_length=max_source_length,\n","        truncation=True,\n","        padding=False,   # để collator pad\n","    )\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            targets,\n","            max_length=max_target_length,\n","            truncation=True,\n","            padding=False,\n","        )[\"input_ids\"]\n","\n","    model_inputs[\"labels\"] = labels\n","    return model_inputs\n","\n","tokenized_datasets = raw_datasets.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","\n","tokenized_datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4DORD6bbLSLv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 10000\n","Dev size: 1000\n"]}],"source":["from datasets import DatasetDict\n","\n","max_train_examples = 10_000\n","max_dev_examples = 1_000\n","\n","small_train = tokenized_datasets[\"train\"].select(range(max_train_examples))\n","small_dev = tokenized_datasets[\"validation\"].select(range(max_dev_examples))\n","\n","small_datasets = DatasetDict({\n","    \"train\": small_train,\n","    \"validation\": small_dev,\n","})\n","\n","print(\"Train size:\", len(small_datasets[\"train\"]))\n","print(\"Dev size:\", len(small_datasets[\"validation\"]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3UgtDPZ4LUJY"},"outputs":[{"data":{"text/plain":["MarianMTModel(\n","  (model): MarianModel(\n","    (shared): Embedding(53685, 512, padding_idx=53684)\n","    (encoder): MarianEncoder(\n","      (embed_tokens): Embedding(53685, 512, padding_idx=53684)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): SiLU()\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (decoder): MarianDecoder(\n","      (embed_tokens): Embedding(53685, 512, padding_idx=53684)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (activation_fn): SiLU()\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=53685, bias=False)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer,\n","    model=model,\n","    label_pad_token_id=-100,\n","    pad_to_multiple_of=8,\n",")\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","batch_size = 8         # với model nhỏ, bs=8 thường vẫn ổn\n","num_epochs = 10\n","\n","train_loader = DataLoader(\n","    small_datasets[\"train\"],\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","\n","dev_loader = DataLoader(\n","    small_datasets[\"validation\"],\n","    batch_size=batch_size,\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","\n","optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n","\n","num_training_steps = num_epochs * len(train_loader)\n","num_warmup_steps = int(0.1 * num_training_steps)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps,\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"98E1TYyKLXfw"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cded092924db439792089bd88d021bce","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","import numpy as np\n","\n","bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [p.strip() for p in preds]\n","    labels = [[l.strip()] for l in labels]\n","    return preds, labels\n","\n","def compute_metrics(eval_pred):\n","    preds, labels = eval_pred\n","\n","    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    preds, labels = postprocess_text(preds, labels)\n","    result = bleu_metric.compute(predictions=preds, references=labels)\n","    return {\"bleu\": result[\"score\"]}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HXip6NvW-a7v"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1jCmjX1BLadA"},"outputs":[],"source":["import math\n","from tqdm.auto import tqdm\n","\n","best_bleu = -1.0\n","train_losses = []\n","train_ppls = []\n","dev_bleus = []\n","\n","for epoch in range(num_epochs):\n","    print(f\"\\n======== Epoch {epoch+1}/{num_epochs} ========\")\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_tokens = 0\n","\n","    # TRAIN\n","    for batch in tqdm(train_loader, desc=\"Training\"):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        optimizer.zero_grad()\n","\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","\n","        labels = batch[\"labels\"]\n","        non_pad = (labels != -100).sum().item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_train_loss += loss.item() * non_pad\n","        total_train_tokens += non_pad\n","\n","    avg_train_loss = total_train_loss / total_train_tokens\n","    train_ppl = math.exp(avg_train_loss)\n","    print(f\"Train loss: {avg_train_loss:.4f} | Train PPL: {train_ppl:.2f}\")\n","    train_losses.append(avg_train_loss)\n","    train_ppls.append(train_ppl)\n","\n","    # EVAL\n","    model.eval()\n","    pred_texts = []\n","    ref_texts = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n","            labels = batch[\"labels\"]\n","\n","            gen_batch = {\n","                \"input_ids\": batch[\"input_ids\"].to(device),\n","                \"attention_mask\": batch[\"attention_mask\"].to(device),\n","            }\n","\n","            generated_tokens = model.generate(\n","                **gen_batch,\n","                max_length=128,\n","                num_beams=5,\n","            )\n","\n","            # decode pred\n","            batch_preds = tokenizer.batch_decode(\n","                generated_tokens,\n","                skip_special_tokens=True\n","            )\n","            pred_texts.extend([p.strip() for p in batch_preds])\n","\n","            # decode ref\n","            label_ids = labels.numpy()\n","            label_ids = np.where(\n","                label_ids != -100,\n","                label_ids,\n","                tokenizer.pad_token_id,\n","            )\n","            batch_refs = tokenizer.batch_decode(\n","                label_ids,\n","                skip_special_tokens=True\n","            )\n","            ref_texts.extend([r.strip() for r in batch_refs])\n","\n","    print(\"len preds:\", len(pred_texts), \"len refs:\", len(ref_texts))\n","    assert len(pred_texts) == len(ref_texts)\n","\n","    refs_for_metric = [[r] for r in ref_texts]\n","\n","    bleu_res = bleu_metric.compute(\n","        predictions=pred_texts,\n","        references=refs_for_metric,\n","    )\n","    bleu = bleu_res[\"score\"]\n","    print(f\"Dev BLEU: {bleu:.2f}\")\n","    dev_bleus.append(bleu)\n","\n","    if bleu \u003e best_bleu:\n","        best_bleu = bleu\n","        print(f\"===\u003e New best BLEU: {best_bleu:.2f}, saving model...\")\n","        model.save_pretrained(MODEL_DIR)\n","        tokenizer.save_pretrained(MODEL_DIR)\n","\n","print(\"Best Dev BLEU:\", best_bleu)\n","print(\"Best model saved at:\", MODEL_DIR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"n2qpKXlV6ST-"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import os\n","\n","# load best model\n","ft_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n","ft_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(device)\n","ft_model.eval()\n","\n","# đọc test EN\n","with open(PUBLIC_TEST_EN, encoding=\"utf-8\") as f:\n","    test_en_lines = [l.strip() for l in f.readlines()]\n","\n","with open(PUBLIC_TEST_VI, encoding=\"utf-8\") as f:\n","    test_vi_lines = [l.strip() for l in f.readlines()]\n","\n","print(\"Số câu test:\", len(test_en_lines))\n","print(\"Ví dụ EN:\", test_en_lines[0])\n","\n","# dịch theo batch\n","from tqdm.auto import tqdm\n","\n","batch_size = 32\n","hypotheses = []\n","\n","for i in tqdm(range(0, len(test_en_lines), batch_size), desc=\"Translating test\"):\n","    batch_src = test_en_lines[i : i + batch_size]\n","\n","    inputs = ft_tokenizer(\n","        batch_src,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=128,\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        output_ids = ft_model.generate(\n","            **inputs,\n","            max_length=128,\n","            num_beams=5,\n","        )\n","\n","    batch_hyp = ft_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n","    hypotheses.extend([h.strip() for h in batch_hyp])\n","\n","print(\"Số câu dịch:\", len(hypotheses))\n","print(\"Ví dụ HYP:\", hypotheses[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ds-GTePVExgN"},"outputs":[],"source":["src = input(\"Nhập câu tiếng Anh cần dịch: \")\n","inputs = ft_tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n","\n","with torch.no_grad():\n","    output_ids = ft_model.generate(\n","        **inputs,\n","        max_length=128,\n","        num_beams=5,\n","    )\n","\n","print(ft_tokenizer.decode(output_ids[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uqczReN5FxTn"},"outputs":[],"source":["import pandas as pd\n","\n","len_src = len(test_en_lines)\n","len_hyp = len(hypotheses)\n","print(\"len_src:\", len_src, \"len_hyp:\", len_hyp)\n","\n","min_len = min(len_src, len_hyp)\n","\n","data = {\n","    \"src_en\": test_en_lines[:min_len],\n","    \"hyp_vi\": hypotheses[:min_len],\n","}\n","\n","# nếu có ref_vi\n","if 'test_vi_lines' in globals():\n","    len_ref = len(test_vi_lines)\n","    print(\"len_ref:\", len_ref)\n","    min_len = min(min_len, len_ref)\n","    data = {\n","        \"src_en\": test_en_lines[:min_len],\n","        \"hyp_vi\": hypotheses[:min_len],\n","        \"ref_vi\": test_vi_lines[:min_len],\n","    }\n","\n","df_test_res = pd.DataFrame(data)\n","\n","df_test_res.to_csv(RESULTS_TSV, sep=\"\\t\", index=False)\n","\n","print(\"Đã lưu:\", RESULTS_TSV, \"với số dòng:\", len(df_test_res))\n","df_test_res.head()\n"]},{"cell_type":"markdown","metadata":{"id":"1umfVtTqH7ml"},"source":["Biểu đồ Loss, Perplexity, BLEU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-z5BCdq5ID6z"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(train_losses) + 1)\n","\n","plt.figure(figsize=(12, 5))\n","\n","# Loss\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, train_losses, marker=\"o\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training Loss\")\n","plt.grid(True)\n","\n","# Perplexity\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, train_ppls, marker=\"o\", color=\"orange\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Perplexity\")\n","plt.title(\"Training Perplexity\")\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9j-JvIwIIOuw"},"outputs":[],"source":["plt.figure(figsize=(6,4))\n","plt.plot(epochs, dev_bleus, marker=\"o\", color=\"green\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"BLEU\")\n","plt.title(\"Dev BLEU Score\")\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"oYe0vBvoiygP"},"source":["Đánh giá lỗi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yMnfCGVNjmfN"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sacrebleu\n","\n","# 1. Tính điểm sentence-level (dùng chrF cho tiếng Việt ổn định hơn)\n","df_test_res['chrf'] = df_test_res.apply(lambda x: sacrebleu.sentence_chrf(x['hyp_vi'], [x['ref_vi']]).score, axis=1)\n","\n","# 2. Tính độ dài và tỷ lệ\n","df_test_res['src_len'] = df_test_res['src_en'].str.split().str.len()\n","df_test_res['hyp_len'] = df_test_res['hyp_vi'].str.split().str.len()\n","df_test_res['ratio'] = df_test_res['hyp_len'] / df_test_res['src_len']\n","\n","# 3. Phân loại lỗi dựa trên ngưỡng (Threshold)\n","def categorize_error(row):\n","    if row['chrf'] \u003e 60: return 'Good'\n","    if row['ratio'] \u003c 0.6: return 'Omission (Dịch thiếu)'\n","    if row['ratio'] \u003e 1.5: return 'Hallucination (Bịa chữ)'\n","    return 'Poor Translation (Sai nghĩa/Ngữ pháp)'\n","\n","df_test_res['error_type'] = df_test_res.apply(categorize_error, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZwOad1K5jsb-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 1. Định nghĩa bảng ánh xạ sang tiếng Anh\n","error_mapping = {\n","    'Dịch tốt': 'Good Translation',\n","    'Dịch thiếu (Omission)': 'Omission',\n","    'Dịch thừa (Hallucination)': 'Hallucination',\n","    'Sai nặng (Wrong Meaning)': 'Critical Error',\n","    'Lỗi ngữ pháp/Từ vựng': 'Linguistic Error'\n","}\n","\n","# 2. Chuẩn bị dữ liệu\n","error_counts = df_test_res['error_type'].value_counts()\n","labels = [error_mapping.get(x, x) for x in error_counts.index]\n","colors = sns.color_palette('pastel')[0:len(labels)]\n","\n","# 3. Vẽ biểu đồ\n","plt.figure(figsize=(10, 8))\n","plt.pie(\n","    error_counts,\n","    labels=labels,\n","    autopct='%1.1f%%',\n","    startangle=140,\n","    colors=colors,\n","    pctdistance=0.85, # Khoảng cách của chữ số %\n","    explode=[0.05] * len(labels) # Tạo khoảng cách nhỏ giữa các miếng\n",")\n","\n","# Vẽ một hình tròn trắng ở giữa để biến nó thành Donut Chart (nhìn hiện đại hơn)\n","centre_circle = plt.Circle((0,0), 0.70, fc='white')\n","fig = plt.gcf()\n","fig.gca().add_artist(centre_circle)\n","\n","plt.title('Machine Translation Error Type Distribution', fontsize=15, fontweight='bold', pad=20)\n","plt.axis('equal')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"M71_Ipg7n0kr"},"outputs":[],"source":["print(\"--- BÁO CÁO THỐNG KÊ LỖI ---\")\n","summary = df_test_res.groupby('error_type').agg({\n","    'chrf': 'mean',\n","    'src_len': 'count'\n","}).rename(columns={'src_len': 'count', 'chrf': 'avg_chrf'})\n","print(summary)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}