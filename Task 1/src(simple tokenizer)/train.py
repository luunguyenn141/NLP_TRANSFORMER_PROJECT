import os
import sys
import time
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt

# Ensure project root is on `sys.path` so imports like `configs` and `src` resolve
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from configs.config import cfg
from src.model.transformer import Transformer
from src.data.data_processing.dataset import TranslationDataset
from src.data.data_processing.vocabulary import Vocabulary

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (Kh·ªõp v·ªõi run_pipeline.py) ---
TOKENIZED_DIR = "src/data/processed/tokenized"
VOCAB_DIR = "src/data/vocab"
CHECKPOINT_DIR = "checkpoints"

os.makedirs(CHECKPOINT_DIR, exist_ok=True)

class EarlyStopping:
    """
    D·ª´ng training s·ªõm n·∫øu validation loss kh√¥ng c·∫£i thi·ªán sau m·ªôt s·ªë epoch nh·∫•t ƒë·ªãnh (patience).
    """
    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):
        """
        Args:
            patience (int): S·ªë epoch ch·ªù ƒë·ª£i sau khi loss kh√¥ng c·∫£i thi·ªán.
            verbose (bool): N·∫øu True, in ra th√¥ng b√°o m·ªói khi validation loss c·∫£i thi·ªán.
            delta (float): Thay ƒë·ªïi t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ª£c coi l√† c·∫£i thi·ªán.
            path (str): ƒê∆∞·ªùng d·∫´n l∆∞u checkpoint model t·ªët nh·∫•t.
            trace_func (function): H√†m d√πng ƒë·ªÉ in th√¥ng b√°o.
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')
        self.delta = delta
        self.path = path
        self.trace_func = trace_func

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            self.trace_func(f'Validation loss kh√¥ng gi·∫£m. EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''L∆∞u model khi validation loss gi·∫£m.'''
        if self.verbose:
            self.trace_func(f'Validation loss gi·∫£m ({self.val_loss_min:.6f} --> {val_loss:.6f}).  ƒêang l∆∞u model...')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss

class NoamOpt:
    """
    Learning rate scheduler with warmup (Transformer style).
    lr = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))
    """
    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=2):
        self.optimizer = optimizer
        self._step = 0
        self.warmup_steps = warmup_steps
        self.factor = factor
        self.d_model = d_model
        self._rate = 0

    def step(self):
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        if step is None:
            step = self._step
        return self.factor * (self.d_model ** (-0.5) *
                              min(step ** (-0.5), step * self.warmup_steps ** (-1.5)))

    def zero_grad(self):
        self.optimizer.zero_grad()

def load_vocab():
    print("‚è≥ ƒêang t·∫£i b·ªô t·ª´ ƒëi·ªÉn...")
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    src_vocab.load(os.path.join(VOCAB_DIR, "src_vocab.json"))
    tgt_vocab.load(os.path.join(VOCAB_DIR, "tgt_vocab.json"))
    return src_vocab, tgt_vocab

def load_data(src_vocab, tgt_vocab, data_type):
    print(f"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu: {data_type}...")
    en_path = os.path.join(TOKENIZED_DIR, f"{data_type}.tok.en")
    vi_path = os.path.join(TOKENIZED_DIR, f"{data_type}.tok.vi")

    with open(en_path, "r", encoding="utf-8") as f:
        src_data = [line.split() for line in f]
    with open(vi_path, "r", encoding="utf-8") as f:
        tgt_data = [line.split() for line in f]

    dataset = TranslationDataset(
        src_data,
        tgt_data,
        src_vocab,
        tgt_vocab,
        max_len=cfg.max_seq_len
        )

    shuffle = True if data_type == "train" else False
    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=shuffle,
                            num_workers=0, collate_fn=dataset.collate_fn, pin_memory=True)
    return dataloader

def train_epoch(model, iterator, optimizer, criterion, device):
    model.train()
    epoch_loss = 0

    for i, batch in enumerate(iterator):
        if isinstance(batch, dict):
            src = batch.get('src')
            if 'trg_input' in batch and 'trg_label' in batch:
                trg_input = batch['trg_input']
                trg_label = batch['trg_label']
                src = src.to(device)
                trg_input = trg_input.to(device)
                trg_label = trg_label.to(device)
            else:
                trg = batch.get('tgt') or batch.get('trg')
                if trg is None:
                    raise KeyError("Batch dict does not contain 'tgt' or 'trg_input'/'trg_label' keys")
                src = src.to(device)
                trg = trg.to(device)
                trg_input = trg[:, :-1]
                trg_label = trg[:, 1:]
        else:
            src, trg = batch[0].to(device), batch[1].to(device)
            trg_input = trg[:, :-1]
            trg_label = trg[:, 1:]

        optimizer.zero_grad()

        output = model(src, trg_input)

        output_dim = output.shape[-1]
        loss = criterion(output.contiguous().view(-1, output_dim),
                         trg_label.contiguous().view(-1))

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()

        epoch_loss += loss.item()

        if i % 100 == 0:
            try:
                current_lr = optimizer.optimizer.param_groups[0]['lr']
            except Exception:
                try:
                    current_lr = optimizer.param_groups[0]['lr']
                except Exception:
                    current_lr = 0.0
            print(f"   Step {i} | Loss: {loss.item():.4f} | LR: {current_lr:.7f}")

    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion, device):
    model.eval()
    epoch_loss = 0

    with torch.no_grad():
        for i, batch in enumerate(iterator):
            if isinstance(batch, dict):
                src = batch.get('src')
                if 'trg_input' in batch and 'trg_label' in batch:
                    trg_input = batch['trg_input']
                    trg_label = batch['trg_label']
                    src = src.to(device)
                    trg_input = trg_input.to(device)
                    trg_label = trg_label.to(device)
                else:
                    trg = batch.get('tgt') or batch.get('trg')
                    if trg is None:
                        raise KeyError("Batch dict does not contain 'tgt' or 'trg_input'/'trg_label' keys")
                    src = src.to(device)
                    trg = trg.to(device)
                    trg_input = trg[:, :-1]
                    trg_label = trg[:, 1:]
            else:
                src, trg = batch[0].to(device), batch[1].to(device)
                trg_input = trg[:, :-1]
                trg_label = trg[:, 1:]

            output = model(src, trg_input)
            output_dim = output.shape[-1]
            loss = criterion(output.contiguous().view(-1, output_dim),
                             trg_label.contiguous().view(-1))

            epoch_loss += loss.item()

    return epoch_loss / len(iterator)


# --- 4. MAIN ---
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán tr√™n thi·∫øt b·ªã: {device}")

    src_vocab, tgt_vocab = load_vocab()
    src_vocab_size = len(src_vocab)
    tgt_vocab_size = len(tgt_vocab)
    print(f"‚úÖ Vocab size: Source={src_vocab_size}, Target={tgt_vocab_size}")

    src_pad_idx = src_vocab.to_index('<pad>')
    trg_pad_idx = tgt_vocab.to_index('<pad>')

    train_loader = load_data(src_vocab, tgt_vocab, "train")
    valid_loader = load_data(src_vocab, tgt_vocab, "tst2012")

    model = Transformer(
        src_vocab_size=src_vocab_size,
        trg_vocab_size=tgt_vocab_size,
        src_pad_idx=src_pad_idx,
        trg_pad_idx=trg_pad_idx,
        d_model=cfg.d_model,
        n_layers=cfg.n_layer,
        n_heads=cfg.n_head,
        d_ff=cfg.d_ff,
        dropout=cfg.dropout,
        max_len=cfg.max_seq_len
    ).to(device)

    base_optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
    model_opt = NoamOpt(base_optimizer, cfg.d_model, warmup_steps=2000, factor=2)

    criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx, label_smoothing=0.1)

    # --- C·∫•u h√¨nh Early Stopping ---
    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')
    # patience=3: D·ª´ng n·∫øu sau 3 epoch validation loss kh√¥ng gi·∫£m
    early_stopping = EarlyStopping(patience=3, verbose=True, path=best_model_path)
    
    # Ki·ªÉm tra checkpoint c≈©
    if os.path.exists(best_model_path):
        print(f"ƒêang n·∫°p l·∫°i checkpoint t·ª´: {best_model_path}")
        try:
            state_dict = torch.load(best_model_path, map_location=device)
            model.load_state_dict(state_dict)
            print("ƒê√£ kh√¥i ph·ª•c tr·∫°ng th√°i m√¥ h√¨nh th√†nh c√¥ng! Ti·∫øp t·ª•c train...")
        except Exception as e:
            print(f"L·ªói khi load checkpoint: {e}")
            print("S·∫Ω train l·∫°i t·ª´ ƒë·∫ßu.")
    else:
        print("Kh√¥ng t√¨m th·∫•y checkpoint c≈©. B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán t·ª´ ƒë·∫ßu.")
        
    
    # Lists ƒë·ªÉ l∆∞u l·ªãch s·ª≠ cho bi·ªÉu ƒë·ªì
    train_losses = []
    valid_losses = []
    train_ppls = []
    valid_ppls = []
    
    print("\nB·∫ÆT ƒê·∫¶U V√íNG L·∫∂P HU·∫§N LUY·ªÜN")
    for epoch in range(cfg.num_epochs):
        start_time = time.time()

        train_loss = train_epoch(model, train_loader, model_opt, criterion, device)
        valid_loss = evaluate(model, valid_loader, criterion, device)
        
        # T√≠nh Perplexity (PPL) = exp(loss)
        train_ppl = math.exp(train_loss)
        valid_ppl = math.exp(valid_loss)

        # --- L∆ØU HISTORY ---
        train_losses.append(train_loss)
        valid_losses.append(valid_loss)
        train_ppls.append(train_ppl)
        valid_ppls.append(valid_ppl)

        end_time = time.time()
        mins, secs = divmod(end_time - start_time, 60)

        print(f'Epoch: {epoch+1:02} | Time: {int(mins)}m {int(secs)}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')

        # --- G·ªåI EARLY STOPPING ---
        # EarlyStopping s·∫Ω t·ª± ƒë·ªông l∆∞u model n·∫øu validation loss gi·∫£m
        early_stopping(valid_loss, model)
        
        if early_stopping.early_stop:
            print("‚õî Early stopping triggered! D·ª´ng hu·∫•n luy·ªán do validation loss kh√¥ng gi·∫£m sau 3 epochs.")
            break

        # L∆∞u checkpoint cu·ªëi c√πng (ƒë·ªÅ ph√≤ng)
        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'last_model.pth'))

    # --- V·∫º BI·ªÇU ƒê·ªí (LOSS & PPL) ---
    print("ƒêang v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Bi·ªÉu ƒë·ªì 1: Loss
    ax1.plot(train_losses, label='Train Loss', color='blue')
    ax1.plot(valid_losses, label='Validation Loss', color='orange')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Loss over Epochs')
    ax1.legend()
    ax1.grid(True)

    # Bi·ªÉu ƒë·ªì 2: Perplexity
    ax2.plot(train_ppls, label='Train PPL', color='green')
    ax2.plot(valid_ppls, label='Validation PPL', color='red')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Perplexity')
    ax2.set_title('Perplexity over Epochs')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    # L∆∞u bi·ªÉu ƒë·ªì th√†nh file ·∫£nh thay v√¨ ch·ªâ show (ti·ªán xem l·∫°i)
    plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_metrics.png'))
    plt.show()
    print(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·∫°i: {os.path.join(CHECKPOINT_DIR, 'training_metrics.png')}")

if __name__ == "__main__":
    main()