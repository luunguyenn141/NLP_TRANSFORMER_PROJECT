import os
import sys
import torch

# --- 1. SETUP ƒê∆Ø·ªúNG D·∫™N IMPORT ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir) # Tr·ªè v·ªÅ th∆∞ m·ª•c g·ªëc NLP_TRANSFORMER_PROJECT
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import c√°c module custom
from configs.config import cfg
from src.model.transformer import Transformer
from src.data.data_processing.vocabulary import Vocabulary
from src.data.data_processing.tokenizer import BPETokenizer

# --- 2. C·∫§U H√åNH ---
# L∆∞u √Ω: C·∫•u tr√∫c th∆∞ m·ª•c ph·∫£i kh·ªõp v·ªõi l√∫c train
VOCAB_DIR = os.path.join(project_root,"NLP_TRANSFORMER_PROJECT", "src", "data", "vocab")
CHECKPOINT_PATH = os.path.join(project_root,"NLP_TRANSFORMER_PROJECT", "checkpoints", "best_model.pth")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 3. H√ÄM BEAM SEARCH ---
def beam_search_decode(model, src, src_mask, max_len, start_symbol, end_symbol, device, beam_width=3):
    model.eval()
    with torch.no_grad():
        enc_src = model.encoder(src, src_mask)
    
    # Beam: list of tuples (cumulative_log_prob, sequence_list)
    beam = [(0.0, [start_symbol])] 
    
    for _ in range(max_len):
        candidates = []
        for score, seq in beam:
            # N·∫øu chu·ªói ƒë√£ k·∫øt th√∫c b·∫±ng end_symbol, gi·ªØ nguy√™n
            if seq[-1] == end_symbol:
                candidates.append((score, seq))
                continue
            
            # Chu·∫©n b·ªã input cho decoder
            trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)
            trg_mask = model.make_trg_mask(trg_tensor)
            
            with torch.no_grad():
                output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)
                # L·∫•y d·ª± ƒëo√°n cho token cu·ªëi c√πng
                prob = output[:, -1, :]
                log_prob = torch.log_softmax(prob, dim=-1)
            
            # L·∫•y top k ·ª©ng vi√™n t·ªët nh·∫•t
            topk_log_probs, topk_indices = torch.topk(log_prob, beam_width)
            
            for i in range(beam_width):
                sym = topk_indices[0][i].item()
                added_score = topk_log_probs[0][i].item()
                candidates.append((score + added_score, seq + [sym]))
        
        # S·∫Øp x·∫øp v√† ch·ªçn ra top beam_width ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t
        beam = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_width]
        
        # N·∫øu t·∫•t c·∫£ c√°c beam ƒë·ªÅu ƒë√£ k·∫øt th√∫c, d·ª´ng s·ªõm
        if all(seq[-1] == end_symbol for _, seq in beam):
            break
            
    # Tr·∫£ v·ªÅ chu·ªói c√≥ ƒëi·ªÉm cao nh·∫•t
    return beam[0][1]

# --- 4. H√ÄM LOAD T√ÄI NGUY√äN (QUAN TR·ªåNG) ---
def load_resources():
    print(f"‚è≥ ƒêang t·∫£i t√†i nguy√™n t·ª´: {VOCAB_DIR}")
    
    # --- A. Load Vocab (Mapping ID <-> Token) ---
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    
    src_vocab_path = os.path.join(VOCAB_DIR, "src_vocab.json")
    tgt_vocab_path = os.path.join(VOCAB_DIR, "tgt_vocab.json")
    
    if not os.path.exists(src_vocab_path) or not os.path.exists(tgt_vocab_path):
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file vocab. H√£y ch·∫°y pipeline tr∆∞·ªõc!")

    src_vocab.load(src_vocab_path)
    tgt_vocab.load(tgt_vocab_path)
    
    # Fix l·ªói <unk> n·∫øu file json b·ªã l·ªói
    if "<unk>" not in src_vocab.stoi:
        src_vocab.stoi["<unk>"] = 1 # Gi·∫£ ƒë·ªãnh index 1, ho·∫∑c len(stoi)
        print("‚ö†Ô∏è C·∫£nh b√°o: ƒê√£ t·ª± ƒë·ªông v√° l·ªói thi·∫øu <unk> trong src_vocab")

    # --- B. Load Tokenizer (BPE Model) ---
    # C·∫ßn 2 tokenizer ri√™ng cho Source (Vi) v√† Target (En)
    src_tokenizer = BPETokenizer(vocab_size=cfg.vocab_size)
    tgt_tokenizer = BPETokenizer(vocab_size=cfg.vocab_size)
    
    src_bpe_path = os.path.join(VOCAB_DIR, "src_bpe.json")
    tgt_bpe_path = os.path.join(VOCAB_DIR, "tgt_bpe.json")
    
    if os.path.exists(src_bpe_path):
        src_tokenizer.load(src_bpe_path)
    else:
        raise FileNotFoundError(f"Thi·∫øu file {src_bpe_path}. H√£y ch·∫°y pipeline step 2!")

    if os.path.exists(tgt_bpe_path):
        tgt_tokenizer.load(tgt_bpe_path)
    else:
        raise FileNotFoundError(f"Thi·∫øu file {tgt_bpe_path}")

    # --- C. Load Model ---
    model = Transformer(
        src_vocab_size=len(src_vocab),
        trg_vocab_size=len(tgt_vocab),
        src_pad_idx=src_vocab.to_index('<pad>'),
        trg_pad_idx=tgt_vocab.to_index('<pad>'),
        d_model=cfg.d_model,
        n_layers=cfg.n_layer,
        n_heads=cfg.n_head,
        d_ff=cfg.d_ff,
        dropout=cfg.dropout,
        max_len=cfg.max_seq_len
    ).to(DEVICE)
    
    if os.path.exists(CHECKPOINT_PATH):
        print(f"‚è≥ ƒêang load checkpoint: {CHECKPOINT_PATH}")
        ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
        
        # Load state_dict v·ªõi strict=False ƒë·ªÉ tr√°nh l·ªói nh·ªè kh√¥ng t∆∞∆°ng th√≠ch
        try:
            model.load_state_dict(ckpt)
            print("‚úÖ Load model th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ö†Ô∏è Load model c√≥ c·∫£nh b√°o (c√≥ th·ªÉ do sai l·ªách k√≠ch th∆∞·ªõc vocab): {e}")
            model.load_state_dict(ckpt, strict=False)
    else:
        print("‚ùå KH√îNG T√åM TH·∫§Y CHECKPOINT! Model s·∫Ω d·ªãch ng·∫´u nhi√™n.")
    
    return model, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer

# --- 5. H√ÄM D·ªäCH ---
def translate_sentence(sentence, model, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer, device):
    model.eval()
    
    # 1. Tokenize (S·ª≠ d·ª•ng BPE tokenizer c·ªßa Source - Ti·∫øng Vi·ªát)
    # tokenize() tr·∫£ v·ªÅ list c√°c sub-words (string)
    src_tokens = src_tokenizer.tokenize(sentence)
    
    # 2. Convert to Indices
    # Map sub-words sang ID d√πng src_vocab
    src_ids = [src_vocab.to_index(t) for t in src_tokens]
    
    # Th√™m batch dimension
    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device) # [1, seq_len]
    
    # 3. T·∫°o Mask
    src_mask = model.make_src_mask(src_tensor)
    
    # 4. Beam Search
    sos_idx = tgt_vocab.to_index('<sos>')
    eos_idx = tgt_vocab.to_index('<eos>')
    
    # K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† list c√°c ID
    pred_ids = beam_search_decode(
        model, src_tensor, src_mask, 
        max_len=cfg.max_seq_len,
        start_symbol=sos_idx, 
        end_symbol=eos_idx, 
        device=device,
        beam_width=3,
        no_repeat_ngram_size=3
    )
    
    # 5. Convert IDs to Text (S·ª≠ d·ª•ng BPE tokenizer c·ªßa Target - Ti·∫øng Anh)
    # Lo·∫°i b·ªè SOS v√† EOS tr∆∞·ªõc khi decode
    pred_ids_clean = [idx for idx in pred_ids if idx not in [sos_idx, eos_idx]]
    
    # C√°ch 1: Map ID -> Token String -> Detokenize (D√πng h√†m detokenize c≈©)
    # pred_tokens = [tgt_vocab.to_token(idx) for idx in pred_ids_clean]
    # translated_text = tgt_tokenizer.detokenize(pred_tokens)

    # C√°ch 2 (Khuy√™n d√πng v·ªõi BPE): D√πng h√†m decode tr·ª±c ti·∫øp c·ªßa th∆∞ vi·ªán
    # Tuy nhi√™n, do c·∫•u tr√∫c project ƒëang t√°ch r·ªùi Vocab v√† Tokenizer, ta d√πng c√°ch 1 cho an to√†n
    # Ho·∫∑c n·∫øu b·∫°n ƒë√£ update detokenize nh∆∞ t√¥i h∆∞·ªõng d·∫´n tr∆∞·ªõc ƒë√≥:
    pred_tokens = [tgt_vocab.to_token(idx) for idx in pred_ids_clean]
    translated_text = tgt_tokenizer.detokenize(pred_tokens)
    
    return translated_text

# --- 6. MAIN ---
def main():
    try:
        model, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer = load_resources()
    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi t·∫°o: {e}")
        return

    print("\n" + "="*50)
    print(f"üåè DEMO D·ªäCH M√ÅY: {cfg.src_lang.upper()} -> {cfg.tgt_lang.upper()}")
    print("Nh·∫≠p 'q' ƒë·ªÉ tho√°t.")
    print("="*50 + "\n")
    
    while True:
        try:
            src_text = input(f"Nh·∫≠p c√¢u ({cfg.src_lang}): ")
            if src_text.lower() in ['q', 'quit', 'exit']:
                break
            
            if not src_text.strip():
                continue
                
            translation = translate_sentence(
                src_text, model, src_vocab, tgt_vocab, 
                src_tokenizer, tgt_tokenizer, DEVICE
            )
            
            print(f"-> D·ªãch ({cfg.tgt_lang}): {translation}")
            print("-" * 30)
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ùå L·ªói khi d·ªãch: {e}")

if __name__ == "__main__":
    main()