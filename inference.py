import os
import sys
import torch
import re

# ƒê·∫£m b·∫£o import ƒë∆∞·ª£c c√°c module trong project
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from configs.config import cfg
from src.model.transformer import Transformer
from src.data.data_processing.vocabulary import Vocabulary
from src.data.data_processing.tokenizer import SimpleTokenizer

# --- C·∫§U H√åNH ---
VOCAB_DIR = "src/data/vocab"
CHECKPOINT_PATH = "checkpoints/best_model.pth" # ƒê∆∞·ªùng d·∫´n model t·ªët nh·∫•t
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- H√ÄM BEAM SEARCH (ƒê∆∞a v√†o ƒë√¢y ƒë·ªÉ ti·ªán ch·∫°y ƒë·ªôc l·∫≠p) ---
def beam_search_decode(model, src, src_mask, max_len, start_symbol, end_symbol, device, beam_width=3):
    model.eval()
    with torch.no_grad():
        enc_src = model.encoder(src, src_mask)
    
    # Beam kh·ªüi t·∫°o: (score, sequence)
    beam = [(0.0, [start_symbol])] 
    
    for _ in range(max_len):
        candidates = []
        for score, seq in beam:
            if seq[-1] == end_symbol:
                candidates.append((score, seq))
                continue
            
            trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)
            trg_mask = model.make_trg_mask(trg_tensor)
            
            with torch.no_grad():
                output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)
                prob = output[:, -1, :]
                log_prob = torch.log_softmax(prob, dim=-1)
            
            topk_log_probs, topk_indices = torch.topk(log_prob, beam_width)
            
            for i in range(beam_width):
                sym = topk_indices[0][i].item()
                added_score = topk_log_probs[0][i].item()
                candidates.append((score + added_score, seq + [sym]))
        
        beam = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_width]
        if all(seq[-1] == end_symbol for _, seq in beam):
            break
            
    return beam[0][1]

# --- H√ÄM X·ª¨ L√ù CH√çNH ---
def load_resources():
    print("‚è≥ ƒêang t·∫£i t√†i nguy√™n (Vocab, Model)...")
    
    # 1. Load Vocab
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    # ƒê∆∞·ªùng d·∫´n file json d·ª±a tr√™n c·∫•u tr√∫c th∆∞ m·ª•c b·∫°n ƒë√£ upload
    src_vocab.load(os.path.join(VOCAB_DIR, "src_vocab.json")) #
    tgt_vocab.load(os.path.join(VOCAB_DIR, "tgt_vocab.json")) #
    
    # 2. Load Tokenizer
    tokenizer = SimpleTokenizer() #
    
    # 3. Load Model
    src_pad_idx = src_vocab.to_index('<pad>')
    trg_pad_idx = tgt_vocab.to_index('<pad>')
    
    model = Transformer(
        src_vocab_size=len(src_vocab),
        trg_vocab_size=len(tgt_vocab),
        src_pad_idx=src_pad_idx,
        trg_pad_idx=trg_pad_idx,
        d_model=cfg.d_model,     #
        n_layers=cfg.n_layer,    #
        n_heads=cfg.n_head,      #
        d_ff=cfg.d_ff,           #
        dropout=cfg.dropout,     #
        max_len=cfg.max_seq_len  #
    ).to(DEVICE)
    
    if os.path.exists(CHECKPOINT_PATH):
        ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
        # Try to load state dict permissively to support older checkpoints
        load_res = model.load_state_dict(ckpt, strict=False)
        # Report any missing / unexpected keys to help debugging
        missing = load_res.missing_keys if hasattr(load_res, 'missing_keys') else load_res.get('missing_keys')
        unexpected = load_res.unexpected_keys if hasattr(load_res, 'unexpected_keys') else load_res.get('unexpected_keys')
        print(f"‚úÖ ƒê√£ load model t·ª´ {CHECKPOINT_PATH}")
        if missing:
            print(f"‚ö†Ô∏è Missing keys in checkpoint (model had these keys but checkpoint did not): {missing}")
        if unexpected:
            print(f"‚ö†Ô∏è Unexpected keys in checkpoint (checkpoint had these keys not used by model): {unexpected}")
    else:
        print(f"‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y {CHECKPOINT_PATH}. Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán!")
    
    return model, src_vocab, tgt_vocab, tokenizer

def translate_input(sentence, model, src_vocab, tgt_vocab, tokenizer, device):
    # 1. Tokenize & Preprocess
    tokens = tokenizer.tokenize(sentence)
    
    # 2. Convert to Indices
    src_indices = [src_vocab.to_index(token) for token in tokens]
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device) # [1, seq_len]
    
    # 3. Create Mask
    src_mask = model.make_src_mask(src_tensor) #
    
    # 4. Beam Search Decoding
    sos_idx = tgt_vocab.to_index('<sos>')
    eos_idx = tgt_vocab.to_index('<eos>')
    
    pred_indices = beam_search_decode(
        model, src_tensor, src_mask, 
        max_len=100, 
        start_symbol=sos_idx, 
        end_symbol=eos_idx, 
        device=device,
        beam_width=5 # B·∫°n c√≥ th·ªÉ ch·ªânh beam_width t·∫°i ƒë√¢y
    )
    
    # 5. Convert Indices to Text
    pred_tokens = [tgt_vocab.to_token(idx) for idx in pred_indices if idx not in [sos_idx, eos_idx]]
    translated_text = tokenizer.detokenize(pred_tokens) #
    
    return translated_text

# --- MAIN LOOP ---
def main():
    model, src_vocab, tgt_vocab, tokenizer = load_resources()
    
    print("\n" + "="*40)
    print("ü§ñ DEMO D·ªäCH M√ÅY (BEAM SEARCH)")
    print("Nh·∫≠p 'q' ho·∫∑c 'quit' ƒë·ªÉ tho√°t.")
    print("="*40 + "\n")
    
    while True:
        try:
            src_text = input("M·ªùi nh·∫≠p c√¢u ti·∫øng Anh: ")
            if src_text.lower() in ['q', 'quit', 'exit']:
                print("T·∫°m bi·ªát!")
                break
            
            if not src_text.strip():
                continue
                
            translation = translate_input(src_text, model, src_vocab, tgt_vocab, tokenizer, DEVICE)
            
            print(f"-> B·∫£n d·ªãch ti·∫øng Vi·ªát: {translation}")
            print("-" * 30)
            
        except KeyboardInterrupt:
            print("\nƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh.")
            break
        except Exception as e:
            print(f"L·ªói: {e}")

if __name__ == "__main__":
    main()